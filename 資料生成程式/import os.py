import os
import json
import requests
import ssl
import re
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import urljoin, urlsplit
import time
from requests.adapters import HTTPAdapter
from urllib3.poolmanager import PoolManager

# --- è‡ªè¨‚ TLS/SSL é©é…å™¨ä»¥æ”¯æ´èˆŠç‰ˆä¼ºæœå™¨ ---
class LegacyTLSAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False, **pool_kwargs):
        context = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)
        context.options |= getattr(ssl, "OP_LEGACY_SERVER_CONNECT", 0x4)
        context.check_hostname = False
        context.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections,
            maxsize=maxsize,
            block=block,
            ssl_context=context,
            **pool_kwargs
        )

# --- å…¨åŸŸå¸¸æ•¸è¨­å®š ---
BASE_ROOT = r"C:\Users\weiwe\Desktop\legislative_ai_web"
BASE_STORAGE = os.path.join(BASE_ROOT, "storage")
JSON_BASE = os.path.join(BASE_STORAGE, "progress")
DOC_BASE = os.path.join(BASE_STORAGE, "doc")
BASE_URL = "https://lis.ly.gov.tw"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# --- å»ºç«‹ä¸¦è¨­å®šå…¨åŸŸ Session ç‰©ä»¶ ---
session = requests.Session()
session.mount('https://', LegacyTLSAdapter())
session.verify = False
requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)


def ensure_dir(path: str):
    """ç¢ºä¿æŒ‡å®šè·¯å¾‘çš„è³‡æ–™å¤¾å­˜åœ¨ï¼Œè‹¥ä¸å­˜åœ¨å‰‡å»ºç«‹ã€‚"""
    os.makedirs(path, exist_ok=True)

def fetch_soup(url: str) -> BeautifulSoup | None:
    """æŠ“å–æŒ‡å®š URL çš„ç¶²é å…§å®¹ä¸¦è§£ææˆ BeautifulSoup ç‰©ä»¶ã€‚"""
    try:
        r = session.get(url, headers=HEADERS, timeout=30)
        r.raise_for_status()
        r.encoding = "utf-8"
        return BeautifulSoup(r.text, "html.parser")
    except requests.exceptions.RequestException as e:
        print(f"æŠ“å–å¤±æ•—ï¼š{url}, éŒ¯èª¤ï¼š{e}")
        return None

def sanitize_filename(name: str) -> str:
    """å°‡å­—ä¸²ä¸­çš„ç„¡æ•ˆå­—å…ƒç§»é™¤æˆ–æ›¿æ›ï¼Œä½¿å…¶å¯ç”¨æ–¼æª”åã€‚"""
    sanitized_name = re.sub(r'[\\/*?:"<>|]', "", name)
    sanitized_name = re.sub(r'\s+', '_', sanitized_name)
    return sanitized_name

def fetch_proposal_number(detail_url: str) -> str | None:
    """å¾è­°æ¡ˆè©³æƒ…é æŠ“å–ææ¡ˆç·¨è™Ÿã€‚"""
    print(f"    -> æ­£åœ¨è¨ªå•è©³æƒ…é æŠ“å–ææ¡ˆç·¨è™Ÿ: {detail_url}")
    detail_soup = fetch_soup(detail_url)
    if not detail_soup: return None

    # ç­–ç•¥1ï¼šå°‹æ‰¾ "ææ¡ˆç·¨è™Ÿ" çš„è¡¨æ ¼æ¬„ä½
    label_td = detail_soup.find("td", string=lambda t: t and t.strip() == "ææ¡ˆç·¨è™Ÿ")
    if label_td and (value_td := label_td.find_next_sibling("td")):
        if proposal_no := value_td.get_text(strip=True):
            print(f"    -> [ç­–ç•¥1æˆåŠŸ] æˆåŠŸæŠ“å–åˆ°ææ¡ˆç·¨è™Ÿ: {proposal_no}")
            return proposal_no

    # ç­–ç•¥2ï¼šä½¿ç”¨æ­£è¦è¡¨é”å¼å¾é é¢ç´”æ–‡å­—ä¸­å°‹æ‰¾
    page_text = detail_soup.get_text()
    match = re.search(r"ææ¡ˆç·¨è™Ÿ\s*([\s\S]+?)\s*(?:æ³•åç¨±|è³‡æ–™ä¾†æº|ç³»çµ±è™Ÿ)", page_text)
    if match:
        proposal_no = match.group(1).strip()
        if proposal_no and len(proposal_no) < 100:
            print(f"    -> [ç­–ç•¥2æˆåŠŸ] æˆåŠŸæŠ“å–åˆ°ææ¡ˆç·¨è™Ÿ: {proposal_no}")
            return proposal_no

    print("    -> âŒ [æ‰€æœ‰ç­–ç•¥å¤±æ•—] æœªèƒ½åœ¨æ­¤é é¢æ‰¾åˆ°ææ¡ˆç·¨è™Ÿã€‚")
    return None

def parse_page_data(soup: BeautifulSoup, current_page_url: str) -> list:
    """è§£æåˆ—è¡¨é ï¼Œæå–æ¯ç­†è­°æ¡ˆçš„è³‡è¨Šã€‚"""
    rows = soup.select("table tr")
    data = []
    if len(rows) <= 1: return data
    
    for row in rows[1:]:
        cols = row.find_all("td")
        if len(cols) < 7: continue
        
        name = cols[2].get_text(strip=True)
        proposer = cols[3].get_text(strip=True)
        full_progress_text = cols[4].get_text(strip=True)
        
        date_txt = ""
        if progress_parts := full_progress_text.split(' ', 1):
            date_txt = progress_parts[0]
            
        doc_links = []
        if doc_icon_img := cols[6].find("img", src="/billtp/images/doc_icon.png"):
            if doc_link_tag := doc_icon_img.find_parent("a"):
                if href := doc_link_tag.get("href"):
                    doc_links.append(urljoin(BASE_URL, href))
        
        proposal_no = None
        if link_tag := cols[5].find("a"):
            if link_tag.find("img"): # åªæ‰¾å¸¶æœ‰åœ–ç‰‡çš„é€£çµ
                if href := link_tag.get("href"):
                    detail_url = urljoin(current_page_url, href)
                    proposal_no = fetch_proposal_number(detail_url)
                    time.sleep(0.5)
        
        if not proposal_no:
             print(f"    -> åœ¨ '{name}' é€™ä¸€è¡Œæ‰¾ä¸åˆ°å¯©è­°é€²åº¦çš„åœ–ç‰‡æŒ‰éˆ•é€£çµã€‚")

        data.append({
            "date": date_txt,
            "bill_name": name,
            "proposer": proposer,
            "proposal_no": proposal_no,
            "progress": full_progress_text,
            "doc_links": doc_links
        })
    return data

def find_next_page_url(soup: BeautifulSoup, current_url: str) -> str | None:
    """åœ¨é é¢ä¸­å°‹æ‰¾ã€Œä¸‹ä¸€é ã€çš„é€£çµã€‚"""
    if next_page_img := soup.find("img", src="/billtp/images/page_next.png"):
        if next_page_link := next_page_img.find_parent("a"):
            if href := next_page_link.get("href"):
                return urljoin(current_url, href)
    return None

def download_docs(item: dict, year: int, month: int):
    """ä¸‹è¼‰èˆ‡è­°æ¡ˆé—œè¯çš„ DOC æª”æ¡ˆã€‚"""
    doc_urls = item.get("doc_links", [])
    if not doc_urls: return
    
    folder = os.path.join(DOC_BASE, f"{year}_{month:02d}")
    ensure_dir(folder)

    base_name = f"{item.get('date', 'æœªçŸ¥æ—¥æœŸ')}_{item.get('proposer', 'æœªçŸ¥ææ¡ˆäºº')}_{item.get('bill_name', 'æœªçŸ¥æ³•æ¡ˆ')}"
    sanitized_base_name = sanitize_filename(base_name)
    fname = f"{sanitized_base_name}.doc"
    path = os.path.join(folder, fname)

    if os.path.exists(path):
        print(f"  æª”æ¡ˆå·²å­˜åœ¨ï¼Œè·³éï¼š{fname}")
        return

    for url in doc_urls:
        try:
            print(f"  æ­£åœ¨ä¸‹è¼‰ï¼š{fname} å¾ {url}")
            rr = session.get(url, headers=HEADERS, stream=True, timeout=60)
            rr.raise_for_status()
            with open(path, "wb") as f:
                for chunk in rr.iter_content(chunk_size=8192):
                    f.write(chunk)
            time.sleep(0.5)
            break # å‡è¨­ä¸€å€‹è­°æ¡ˆåªéœ€è¦ä¸‹è¼‰ä¸€å€‹æª”æ¡ˆ
        except requests.exceptions.RequestException as e:
            print(f"  ä¸‹è¼‰å¤±æ•—ï¼š{url}, éŒ¯èª¤ï¼š{e}")
        except Exception as e:
            print(f"  è™•ç†æª”æ¡ˆæ™‚ç™¼ç”ŸæœªçŸ¥éŒ¯èª¤ï¼š{url}, éŒ¯èª¤ï¼š{e}")

def crawl_category(cat_name: str, start_url: str, target_year: int, target_month: int):
    """
    çˆ¬å–æŒ‡å®šåˆ†é¡çš„è­°æ¡ˆï¼Œä¸¦æ¡ç”¨æ›´å¼·å¥çš„ç¿»é é‚è¼¯ã€‚
    """
    print(f"--- é–‹å§‹çˆ¬å–åˆ†é¡ï¼š{cat_name} ---")
    all_filtered_items = []
    current_url = start_url
    
    # ç›®æ¨™æœˆä»½çš„ç¬¬ä¸€å¤©ï¼Œç”¨æ–¼æ—¥æœŸæ¯”è¼ƒ
    target_month_start_date = datetime(target_year, target_month, 1)

    while current_url:
        print(f"æ­£åœ¨è™•ç†é é¢ï¼š{current_url}")
        soup = fetch_soup(current_url)
        if not soup: break
        
        page_data = parse_page_data(soup, current_url)
        if not page_data:
            print("  -> é é¢ç„¡è³‡æ–™ï¼Œå¯èƒ½å·²åˆ°æœ«é æˆ–è§£æå¤±æ•—ã€‚")
            break

        items_found_on_this_page = 0
        # å…ˆå‡è¨­æ­¤é è³‡æ–™éƒ½æ¯”ç›®æ¨™æœˆä»½èˆŠï¼Œå¦‚æœåœ¨è¿´åœˆä¸­æ‰¾åˆ°ä»»ä½•ä¸€ç­†è³‡æ–™æ˜¯åœ¨ç›®æ¨™æœˆä»½æˆ–ä¹‹å¾Œï¼Œå°±è¨­ç‚º False
        is_page_too_old = True

        for item in page_data:
            date_str = item["date"]
            try:
                if len(date_str) != 7 or not date_str.isdigit(): continue
                minguo_year = int(date_str[0:3])
                month = int(date_str[3:5])
                day = int(date_str[5:7])
                gregorian_year = minguo_year + 1911
                item_date = datetime(gregorian_year, month, day)

                # åªè¦æœ‰ä¸€ç­†è³‡æ–™çš„æ—¥æœŸ >= ç›®æ¨™æœˆä»½çš„ç¬¬ä¸€å¤©ï¼Œå°±è¡¨ç¤ºæˆ‘å€‘éœ€è¦ç¹¼çºŒç¿»é 
                if item_date >= target_month_start_date:
                    is_page_too_old = False

                # æª¢æŸ¥è³‡æ–™æ˜¯å¦ç¬¦åˆæŒ‡å®šçš„å¹´å’Œæœˆ
                if gregorian_year == target_year and month == target_month:
                    all_filtered_items.append(item)
                    items_found_on_this_page += 1

            except (ValueError, TypeError):
                print(f"  -> ç„¡æ³•è§£æçš„æ—¥æœŸæ ¼å¼ï¼Œè·³éï¼š'{date_str}'")
                continue
        
        if items_found_on_this_page > 0:
            print(f"  âœ… åœ¨æ­¤é æ‰¾åˆ° {items_found_on_this_page} ç­†ç¬¦åˆ {target_year}/{target_month:02d} çš„è³‡æ–™ã€‚")
        elif not is_page_too_old:
            print(f"  â„¹ï¸  æ­¤é é¢ç„¡ç¬¦åˆ {target_year}/{target_month:02d} çš„è³‡æ–™ï¼Œä½†å› æ—¥æœŸè¼ƒæ–°ï¼Œå°‡ç¹¼çºŒç¿»é ...")

        # æ ¸å¿ƒåœæ­¢æ¢ä»¶ï¼šå¦‚æœæ­¤é é¢æ‰€æœ‰è³‡æ–™éƒ½æ¯”ç›®æ¨™æœˆä»½é‚„èˆŠï¼Œå°±å¯ä»¥åœæ­¢äº†
        if is_page_too_old:
            print(f"  ğŸ›‘ æ­¤é æ‰€æœ‰è³‡æ–™æ—¥æœŸå‡æ—©æ–¼ {target_year}/{target_month:02d}ï¼Œåœæ­¢åœ¨æ­¤åˆ†é¡ä¸‹ç¹¼çºŒç¿»é ã€‚")
            break

        # å°‹æ‰¾ä¸‹ä¸€é 
        current_url = find_next_page_url(soup, current_url)
        if current_url:
            print("  -> æ‰¾åˆ°ä¸‹ä¸€é ï¼Œæº–å‚™å‰å¾€...")
            time.sleep(1)
        else:
            print("  -> æ‰¾ä¸åˆ°ä¸‹ä¸€é ï¼Œæ­¤åˆ†é¡çˆ¬å–çµæŸã€‚")
    
    if not all_filtered_items:
        print(f"åˆ†é¡ {cat_name} åœ¨ {target_year}/{target_month:02d} æ²’æœ‰æ‰¾åˆ°ä»»ä½•è³‡æ–™ã€‚")
        return

    json_folder = os.path.join(JSON_BASE, f"{target_year}_{target_month:02d}")
    ensure_dir(json_folder)
    json_fn = os.path.join(json_folder, f"{target_year}_{target_month:02d}_{cat_name}.json")
    print(f"å¯«å…¥ JSON æª”æ¡ˆï¼š{json_fn}")
    with open(json_fn, "w", encoding="utf-8") as jf:
        json.dump(all_filtered_items, jf, ensure_ascii=False, indent=4)
    
    print(f"é–‹å§‹ä¸‹è¼‰ {cat_name} åˆ†é¡çš„é—œè¯æª”æ¡ˆ...")
    for item in all_filtered_items:
        download_docs(item, target_year, target_month)
    
    print(f"--- åˆ†é¡ {cat_name} çˆ¬å–å®Œæˆ ---\n")

def main():
    """ä¸»åŸ·è¡Œå‡½å¼ - æ”¹ç‚ºäº’å‹•å¼å•ç­”ç²å–å¹´/æœˆåŠæ‰€æœ‰URL"""
    
    # --- æ­¥é©Ÿ 1ï¼šäº’å‹•å¼å•ç­”ä»¥ç²å–å¹´æœˆä»½ ---
    target_year = 0
    target_month = 0
    now = datetime.now()

    while True:
        try:
            year_input = input(f"â¡ï¸ è«‹è¼¸å…¥è¦æŠ“å–çš„å¹´ä»½ (è¥¿å…ƒï¼Œä¾‹å¦‚ {now.year}): ")
            if not year_input.strip():
                 print("âš ï¸ å¹´ä»½ä¸å¯ç‚ºç©ºï¼Œè«‹é‡æ–°è¼¸å…¥ã€‚")
                 continue
            target_year = int(year_input)
            if 1912 <= target_year <= now.year + 1:
                break
            else:
                print(f"âš ï¸ å¹´ä»½ä¼¼ä¹ä¸æ­£ç¢ºï¼Œè«‹è¼¸å…¥ä»‹æ–¼ 1912 (æ°‘åœ‹å…ƒå¹´) è‡³ {now.year + 1} ä¹‹é–“çš„å¹´ä»½ã€‚")
        except ValueError:
            print("âŒ è¼¸å…¥æ ¼å¼éŒ¯èª¤ï¼Œè«‹è¼¸å…¥è¥¿å…ƒå¹´ä»½çš„æ•¸å­—ã€‚")

    while True:
        try:
            month_input = input("â¡ï¸ è«‹è¼¸å…¥è¦æŠ“å–çš„æœˆä»½ (1-12): ")
            if not month_input.strip():
                print("âš ï¸ æœˆä»½ä¸å¯ç‚ºç©ºï¼Œè«‹é‡æ–°è¼¸å…¥ã€‚")
                continue
            target_month = int(month_input)
            if 1 <= target_month <= 12:
                break
            else:
                print("âš ï¸ æœˆä»½ç„¡æ•ˆï¼Œè«‹è¼¸å…¥ 1 åˆ° 12 ä¹‹é–“çš„æ•¸å­—ã€‚")
        except ValueError:
            print("âŒ è¼¸å…¥æ ¼å¼éŒ¯èª¤ï¼Œè«‹è¼¸å…¥æœˆä»½çš„æ•¸å­—ã€‚")
    
    print(f"\n*** æ‚¨æŒ‡å®šçš„ç›®æ¨™å¹´æœˆç‚º: {target_year}å¹´ {target_month}æœˆ ***")

    # --- æ­¥é©Ÿ 2ï¼šäº’å‹•å¼å•ç­”ä»¥ç²å–å„åˆ†é¡çš„ç¶²å€ ---
    print("\n--- ç¾åœ¨ï¼Œè«‹æä¾›æœ€æ–°çš„åˆ†é¡ç¶²å€ ---")
    print("æ“ä½œæŒ‡å¼•ï¼šè«‹é–‹å•Ÿç€è¦½å™¨é€²å…¥ç«‹æ³•é™¢ã€Œè­°æ¡ˆé€²åº¦æŸ¥è©¢ã€é é¢ï¼Œ")
    print("          ç„¶å¾Œåˆ†åˆ¥é»æ“Šå„å€‹åˆ†é¡ï¼Œå°‡ç€è¦½å™¨ç¶²å€åˆ—çš„ç¶²å€å®Œæ•´è¤‡è£½å¾Œï¼Œè²¼åˆ°ä¸‹æ–¹çš„æç¤ºä¸­ã€‚")

    category_urls = {}
    # ç‚ºäº†è®“æç¤ºæ›´æ¸…æ™°ï¼Œå°‡ç¶²ç«™ä¸Šçš„åç¨±ä¹Ÿä¸€ä½µæ”¾å…¥
    categories_to_ask = {
        "First_Reading": "ä¸€è®€ï¼ˆä»˜å§”å¯©æŸ¥ï¼‰",
        "Committee": "å§”å“¡æœƒå¯©è­°",
        "Second_Reading": "äºŒè®€æœƒ",
        "Third_Reading": "ä¸‰è®€æœƒ",
        "Other": "å…¶ä»–"
    }

    for key, name in categories_to_ask.items():
        while True:
            url_input = input(f"â¡ï¸ è«‹è¼¸å…¥ã€Œ{name}ã€çš„ç¶²å€: ")
            # ç°¡å–®é©—è­‰ç¶²å€æ ¼å¼
            if url_input.strip().lower().startswith(('http://', 'https://')):
                category_urls[key] = url_input.strip()
                break
            else:
                print("âŒ ç¶²å€æ ¼å¼ä¼¼ä¹ä¸æ­£ç¢ºï¼Œè«‹ç¢ºä¿ä»¥ http:// æˆ– https:// é–‹é ­ï¼Œç„¶å¾Œå†è©¦ä¸€æ¬¡ã€‚")

    print("\nâœ… æ„Ÿè¬æ‚¨æä¾›ç¶²å€ï¼Œå³å°‡é–‹å§‹çˆ¬å–...\n")
    
    # --- æ­¥é©Ÿ 3ï¼šé–‹å§‹åŸ·è¡Œçˆ¬å– ---
    ensure_dir(JSON_BASE)
    ensure_dir(DOC_BASE)
    
    for cname, url in category_urls.items():
        crawl_category(cname, url, target_year, target_month)
        
    print("âœ… æ‰€æœ‰åˆ†é¡çˆ¬å–å®Œæˆï¼")


if __name__ == "__main__":
    main()